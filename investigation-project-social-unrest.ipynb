{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-19T09:06:34.439275Z","iopub.execute_input":"2023-09-19T09:06:34.439769Z","iopub.status.idle":"2023-09-19T09:06:34.484035Z","shell.execute_reply.started":"2023-09-19T09:06:34.439734Z","shell.execute_reply":"2023-09-19T09:06:34.482696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ****2021 South African Unrest****\n\nThe 2021 South African unrest, also referred to as the July 2021 riots, occurred in South Africa's KwaZulu-Natal and Gauteng provinces from \n**July 9 to 18, 2021**. \n\nThe wave of civil unrest was ignited by the imprisonment of former President Jacob Zuma for contempt of court.","metadata":{}},{"cell_type":"code","source":"csv_file_path= '/kaggle/input/all-days-extracted-tweets-unrest-2021/Extracted_south_african_protest_7.csv'\nunrest_tweets2 = pd.read_csv(csv_file_path, low_memory=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:06:34.487870Z","iopub.execute_input":"2023-09-19T09:06:34.488703Z","iopub.status.idle":"2023-09-19T09:06:34.640735Z","shell.execute_reply.started":"2023-09-19T09:06:34.488658Z","shell.execute_reply":"2023-09-19T09:06:34.638915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unrest_tweets2.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:06:34.642572Z","iopub.execute_input":"2023-09-19T09:06:34.643020Z","iopub.status.idle":"2023-09-19T09:06:34.695805Z","shell.execute_reply.started":"2023-09-19T09:06:34.642983Z","shell.execute_reply":"2023-09-19T09:06:34.693954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('There are {} rows and {} columns in the twitter dataset'.format(unrest_tweets2.shape[0],unrest_tweets2.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:06:34.697153Z","iopub.execute_input":"2023-09-19T09:06:34.698037Z","iopub.status.idle":"2023-09-19T09:06:34.705623Z","shell.execute_reply.started":"2023-09-19T09:06:34.698001Z","shell.execute_reply":"2023-09-19T09:06:34.704091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import DateFormatter\n\n# Assuming earthquake_tweets is your DataFrame with the 'date' column\nunrest_tweets2['date'] = pd.to_datetime(unrest_tweets2[\"Tweet Created At\"])  # Convert the 'date' column to datetime\n\n# Resample the data by day and count the number of tweets\ntweets_per_day = unrest_tweets2.resample('D', on='date').size()\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\n# Plot the data\nax.plot(tweets_per_day.index, tweets_per_day.values, marker='o')\n\n# Set the x-axis format to display only the date\ndate_formatter = DateFormatter('%Y-%m-%d')\nax.xaxis.set_major_formatter(date_formatter)\n\n# Set the dates starting from the beginning of the event:\nspecific_dates = [pd.Timestamp('2021-07-09')]  \ndate_range = pd.date_range(start='2021-07-10', end='2021-07-18', freq='D')\nall_dates = specific_dates + date_range.tolist() \n\nax.set_xticks(all_dates)\n\n# # Ensure that the specific date '2023-02-06' is displayed on the x-axis\n# ax.axvline(pd.Timestamp('2023-02-06'), color='red', linestyle='--', label='2023-02-06')\n# ax.legend()\n\n# Rotate the x-axis labels for better visibility\nplt.xticks(rotation=90)\n\n# Set labels and title\nax.set_xlabel('Date')\nax.set_ylabel('Number of Tweets')\nax.set_title('Number of Tweets per Day')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:06:34.709202Z","iopub.execute_input":"2023-09-19T09:06:34.709582Z","iopub.status.idle":"2023-09-19T09:06:36.521040Z","shell.execute_reply.started":"2023-09-19T09:06:34.709530Z","shell.execute_reply":"2023-09-19T09:06:36.519655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Location**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Count non-null values in the \"Tweet Coordinates\" column\nnon_null_count = unrest_tweets2[\"Tweet Coordinates\"].count()\n\n# Display the count\nprint(\"Number of non-null values in 'Tweet Coordinates':\", non_null_count)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:06:36.522630Z","iopub.execute_input":"2023-09-19T09:06:36.523777Z","iopub.status.idle":"2023-09-19T09:06:36.531897Z","shell.execute_reply.started":"2023-09-19T09:06:36.523738Z","shell.execute_reply":"2023-09-19T09:06:36.530108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Print the first 5 rows of the \"User Location\" column\nprint(\"Sample of 'User Location' column:\")\nprint(unrest_tweets2[\"User Location\"].head())\n\n# Check the datatype of the \"User Location\" column\ndata_type = unrest_tweets2[\"User Location\"].dtype\nprint(\"\\nDatatype of 'User Location' column:\", data_type)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:06:36.533972Z","iopub.execute_input":"2023-09-19T09:06:36.535807Z","iopub.status.idle":"2023-09-19T09:06:36.551213Z","shell.execute_reply.started":"2023-09-19T09:06:36.535750Z","shell.execute_reply":"2023-09-19T09:06:36.549726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Count non-null values in the \"Tweet Coordinates\" column\nnon_null_count = unrest_tweets2[\"User Location\"].count()\n\n# Display the count\nprint(\"Number of non-null values in 'User Location':\", non_null_count)\n\n# Filter rows with non-null values in the \"User Location\" column\nnon_null_user_location_df = unrest_tweets2[unrest_tweets2[\"User Location\"].notnull()]\n\n# Check the size (number of rows and columns)\nnum_rows, num_columns = non_null_user_location_df.shape\nprint(\"Number of rows:\", num_rows)\nprint(\"Number of columns:\", num_columns)\n\n# Check for null values in the \"User Location\" column\nnull_values_count = non_null_user_location_df[\"User Location\"].isnull().sum()\nprint(\"Number of null values in 'User Location' column:\", null_values_count)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:06:36.552827Z","iopub.execute_input":"2023-09-19T09:06:36.553226Z","iopub.status.idle":"2023-09-19T09:06:36.573529Z","shell.execute_reply.started":"2023-09-19T09:06:36.553151Z","shell.execute_reply":"2023-09-19T09:06:36.571935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Test code for API to resolve locations into longitude and latitute:**","metadata":{}},{"cell_type":"code","source":"# import os\n# import pandas as pd\n# from geopy.geocoders import Nominatim\n\n# # Define the geocoding function\n# def geocode_location(location):\n#     geolocator = Nominatim(user_agent=\"Investigation Project\")\n#     try:\n#         location = geolocator.geocode(location)\n#         return (location.longitude, location.latitude)\n#     except AttributeError:\n#         return None\n\n# # Test the geocoding function with a sample location\n# sample_location = \"South Africa, Cape Town\"\n\n# # Call the geocoding function and print the result\n# result = geocode_location(sample_location)\n\n# if result is not None:\n#     longitude, latitude = result\n#     print(f\"Longitude: {longitude}, Latitude: {latitude}\")\n\n#     # Create a DataFrame from the geocoding result\n#     data = {'Location': [sample_location], 'Longitude': [longitude], 'Latitude': [latitude]}\n#     df = pd.DataFrame(data)\n\n#     # Define the output directory\n#     output_dir = '/kaggle/working/'\n\n#     # Save the DataFrame to a CSV file in the output directory\n#     output_filename = 'geocoding_result.csv'\n#     output_filepath = os.path.join(output_dir, output_filename)\n#     df.to_csv(output_filepath, index=False)\n\n#     print(f\"Geocoding result saved to {output_filepath}\")\n# else:\n#     print(\"Location not found or there was an issue with geocoding.\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:06:36.575847Z","iopub.execute_input":"2023-09-19T09:06:36.576414Z","iopub.status.idle":"2023-09-19T09:06:37.583983Z","shell.execute_reply.started":"2023-09-19T09:06:36.576342Z","shell.execute_reply":"2023-09-19T09:06:37.582456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The following code is used to query the Nominatim API to resolve the locations to longitudes and latitude for plotting. However, within the data, errors arose from the queries, so the data has been split into blocks to isolate which part of the data is corrupted/producing the query error. Each block of the overall data is stored in a csv which in turn will be combined.**","metadata":{}},{"cell_type":"code","source":"# import os\n# import pandas as pd\n# from geopy.geocoders import Nominatim\n# from geopy.exc import GeocoderQueryError\n\n# def geocode_and_save_block(user_locations, block_number, batch_size=100):\n#     # Define a function to geocode user locations to latitude and longitude\n#     def geocode_location_batch(locations, batch_size=batch_size):\n#         geolocator = Nominatim(user_agent=\"SA Tweets from Social Unrest\")\n#         total_locations = len(locations)\n#         geocoded_data = []\n        \n#         # Define a counter to keep track of successful geocoded locations\n#         geocoded_count = 0\n\n#         for i in range(0, total_locations, batch_size):\n#             batch_locations = locations[i:i + batch_size]\n#             batch_results = []\n\n#             for location in batch_locations:\n#                 try:\n#                     result = geolocator.geocode(location, timeout=10)  # Increase timeout\n#                     if result:\n#                         batch_results.append((location, result.longitude, result.latitude))\n#                         geocoded_count += 1\n#                 except (AttributeError, GeocoderQueryError) as e:\n#                     print(f\"Error geocoding location: {location} - {str(e)}\")\n\n#             geocoded_data.extend(batch_results)\n#             print(f\"Geocoded {geocoded_count} of {total_locations} locations.\")\n\n#         return geocoded_data\n\n#     # Split the data into four blocks\n#     total_data = len(user_locations)\n#     block_size = total_data // 4  # Divide the data into 4 equal blocks\n\n#     if block_number not in [1, 2, 3, 4]:\n#         raise ValueError(\"Block number should be 1, 2, 3, or 4.\")\n\n#     # Choose the specified block\n#     block = user_locations[(block_number - 1) * block_size:block_number * block_size]\n\n#     # Batch geocode the specified block of user locations\n#     geocoded_results = geocode_location_batch(block)\n\n#     # Create a DataFrame from the geocoded results\n#     geocoded_df = pd.DataFrame(geocoded_results, columns=[\"User Location\", \"Longitude\", \"Latitude\"])\n\n#     # Define the output CSV file path\n#     output_directory = '/kaggle/working/'\n#     output_filename = f'block{block_number}_geocoding_data.csv'\n#     output_filepath = os.path.join(output_directory, output_filename)\n\n#     # Save the geocoded data to a CSV file\n#     geocoded_df.to_csv(output_filepath, index=False)\n#     print(f\"Geocoded data saved to {output_filepath}\")\n\n#     print(f\"Geocoding completed successfully for block {block_number}.\")\n\n# # Extract the 'User Location' column as a list\n# user_locations = non_null_user_location_df[\"User Location\"].tolist()\n\n# # Specify which block to geocode and save (e.g., block_number=1)\n# geocode_and_save_block(user_locations, block_number=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:56:37.719443Z","iopub.execute_input":"2023-09-19T09:56:37.719994Z","iopub.status.idle":"2023-09-19T10:08:14.195336Z","shell.execute_reply.started":"2023-09-19T09:56:37.719956Z","shell.execute_reply":"2023-09-19T10:08:14.194213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# Define the directory path where the CSV files are located\ndirectory = '/kaggle/input/all-geocoded-data/'\n\n# Define the file names of the CSV files\nfile_names = [\n    'block1_geocoding_data.csv',\n    'block2_geocoding_data.csv',\n    'block3_geocoding_data.csv',\n    'block4_geocoding_data.csv'\n]\n\n# Initialize an empty list to store DataFrames\ndata_frames = []\n\n# Define a function to preprocess the CSV data\ndef preprocess_csv(file_path):\n    with open(file_path, 'r') as file:\n        lines = file.readlines()\n\n    # Initialize lists to store valid longitude and latitude values\n    valid_longitude = []\n    valid_latitude = []\n\n    for line in lines[1:]:\n        values = line.strip().split(',')\n        # Check if there are at least 2 values (longitude and latitude)\n        if len(values) >= 2:\n            # Attempt to convert longitude and latitude to floats\n            try:\n                longitude = float(values[-2])\n                latitude = float(values[-1])\n                valid_longitude.append(longitude)\n                valid_latitude.append(latitude)\n            except ValueError:\n                # Skip lines with invalid longitude or latitude values\n                continue\n\n    # Create a DataFrame from the valid longitude and latitude values\n    df = pd.DataFrame({'Longitude': valid_longitude, 'Latitude': valid_latitude})\n\n    return df\n\nfor file_name in file_names:\n    print(f\"Reading file: {file_name}\")\n    file_path = directory + file_name\n    # Preprocess the CSV file\n    df = preprocess_csv(file_path)\n    data_frames.append(df)\n\n# Concatenate the DataFrames vertically to combine them\ncombined_df = pd.concat(data_frames, ignore_index=True)\n\n# Display key features of the combined DataFrame\nprint(\"Head of the Combined DataFrame:\")\nprint(combined_df.head())\n\nprint(\"\\nNumber of Rows and Columns in the Combined DataFrame:\")\nprint(combined_df.shape)\n\n# Now, you have the combined DataFrame containing only valid longitude and latitude.","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:56:48.016830Z","iopub.execute_input":"2023-09-19T10:56:48.018528Z","iopub.status.idle":"2023-09-19T10:56:48.052904Z","shell.execute_reply.started":"2023-09-19T10:56:48.018470Z","shell.execute_reply":"2023-09-19T10:56:48.051261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import geopandas as gpd\nfrom shapely.geometry import Point\nimport matplotlib.pyplot as plt\n\n\n# Create Point geometries using Shapely\ngeometry = [Point(xy) for xy in zip(combined_df['Longitude'], combined_df['Latitude'])]\n\n# Create a GeoDataFrame\ncrs = 'EPSG:4326'  # Assuming the coordinates are in WGS 84\ngeo_df = gpd.GeoDataFrame(combined_df, crs=crs, geometry=geometry)\n\n# Download world shapefile from GeoPandas datasets (if not already downloaded)\n# gpd.datasets.get_path('naturalearth_lowres').to_csv(\"path_to_shapefile.zip\")\n \n# Load world shapefile\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\n# Create subplots\nfig, ax = plt.subplots(1, figsize=(16, 8), facecolor='lightblue')\n\n# Plot the world map with white color\nworld.plot(ax=ax, color='white', edgecolor='black')\n\n# Plot your geo_df data\ngeo_df.plot(ax=ax, markersize=1, color='m', marker='o')\n\n# Turn off the axis\nax.axis('off')\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T11:00:04.124282Z","iopub.execute_input":"2023-09-19T11:00:04.124716Z","iopub.status.idle":"2023-09-19T11:00:06.866052Z","shell.execute_reply.started":"2023-09-19T11:00:04.124684Z","shell.execute_reply":"2023-09-19T11:00:06.864830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import geopandas as gpd\nfrom shapely.geometry import Point\nimport matplotlib.pyplot as plt\n\n\n# Create Point geometries using Shapely\ngeometry = [Point(xy) for xy in zip(combined_df['Longitude'], combined_df['Latitude'])]\n\n# Create a GeoDataFrame\ncrs = 'EPSG:4326'  # Assuming the coordinates are in WGS 84\ngeo_df = gpd.GeoDataFrame(combined_df, crs=crs, geometry=geometry)\n\n# Load South African provinces shapefile\nsa_provinces = gpd.read_file('/kaggle/input/natural-earth-provinces/ne_10m_admin_1_states_provinces.shp')\n\n# Filter the shapefile to get only South Africa\nsouth_africa = sa_provinces[sa_provinces['admin'] == 'South Africa']\n\n# Get the bounding box of South Africa\nsa_bbox = south_africa.bounds\n\n# Set the map extent to cover South Africa's bounding box\nfig, ax = plt.subplots(1, figsize=(16, 8), facecolor='lightblue')\nax.set_xlim([sa_bbox.minx.min(), sa_bbox.maxx.max()])\nax.set_ylim([sa_bbox.miny.min(), sa_bbox.maxy.max()])\n\n# Plot the South African provinces\nsouth_africa.boundary.plot(ax=ax, linewidth=2, color='black')\n\n# Plot your geo_df data within the bounds of South Africa\ngeo_df.plot(ax=ax, markersize=1, color='m', marker='o')\n\n# Turn off the axis\nax.axis('off')\n\n# Show the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T11:00:44.183073Z","iopub.execute_input":"2023-09-19T11:00:44.183577Z","iopub.status.idle":"2023-09-19T11:01:16.002210Z","shell.execute_reply.started":"2023-09-19T11:00:44.183519Z","shell.execute_reply":"2023-09-19T11:01:16.000420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  **Global Database of Events, Language, and Tone (GDELT)**\n \nThe Global Database of Events, Language, and Tone (GDELT) is a comprehensive and continuously updated dataset that monitors and records various global events, news articles, and media sources from around the world. GDELT's primary purpose is to provide a vast repository of structured data that researchers, analysts, and data scientists can use to analyze and gain insights into global events, trends, and sentiments.\n​\nKey features of GDELT include:\n​\n- **Event Data**: GDELT captures a wide range of events, including political, economic, social, and cultural events, across different countries and regions.\n​\n- **Media Monitoring**: GDELT scans thousands of news articles, broadcasts, and online sources in multiple languages to extract valuable information.\n​\n- **Sentiment Analysis**: It includes sentiment analysis and tone indicators, helping to understand the emotional context of news and events.\n​\n- **Temporal Coverage**: GDELT's data goes back several decades, allowing users to explore historical trends and patterns.\n​\n- **Geospatial Information**: The dataset includes geospatial information, enabling the mapping of events and their locations.\n​\nhttps://www.gdeltproject.org/\n​\n  Cell In[18], line 3\n    The Global Database of Events, Language, and Tone (GDELT) is a comprehensive and continuously updated dataset that monitors and records various global events, news articles, and media sources from around the world. GDELT's primary purpose is to provide a vast repository of structured data that researchers, analysts, and data scientists can use to analyze and gain insights into global events, trends, and sentiments.\n                                                                                                                                                                                                               ","metadata":{}},{"cell_type":"markdown","source":"# Summary of Format and shape of Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n \n\n# Define the directory and file name\ndirectory = \"south-africa-gdelts-riots-2021\"\nfile_name = \"Query_riots_2021_test6_allColumns.csv\"\n\n \n\n# Create the full file path\nfile_path = \"/kaggle/input/south-africa-gdelt-riots-2021/Query_roits_2021_test6_allColumns.csv\"\n\n \n\n# Load the dataset into a Pandas DataFrame\ntry:\n    df = pd.read_csv(file_path)\n    \n    # Display the main specifications of the dataset\n    print(\"Dataset Specifications:\")\n    print(f\"File Path: {file_path}\")\n    print(f\"Number of Rows: {len(df)}\")\n    print(f\"Number of Columns: {len(df.columns)}\")\n    print(\"Column Names:\")\n    for column in df.columns:\n        print(f\" - {column}\")\n    print(\"Data Types:\")\n    for column, dtype in df.dtypes.items():\n        print(f\" - {column}: {dtype}\")\n#     print(\"Summary Statistics:\")\n#     print(df.describe())\nexcept FileNotFoundError:\n    print(f\"File '{file_path}' not found.\")\nexcept Exception as e:\n    print(f\"An error occurred: {str(e)}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:07:17.474940Z","iopub.status.idle":"2023-09-19T09:07:17.475472Z","shell.execute_reply.started":"2023-09-19T09:07:17.475229Z","shell.execute_reply":"2023-09-19T09:07:17.475255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plot number of datapoints per day (9 July 2021 (start of unrest) to 21 July (3 days after unrest ended))**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\nfile_path = \"/kaggle/input/south-africa-gdelt-riots-2021/Query_roits_2021_test6_allColumns.csv\"\ndf = pd.read_csv(file_path)\n\n# Group the data by SQLDATE and count the number of data points for each date\ndate_counts = df['SQLDATE'].value_counts().sort_index()\n\n# Create a bar graph\nplt.figure(figsize=(12, 6))\ndate_counts.plot(kind='bar', color='skyblue')\nplt.title('Number of articles per day')\nplt.xlabel('DATE')\nplt.ylabel('Count')\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:07:17.476924Z","iopub.status.idle":"2023-09-19T09:07:17.477316Z","shell.execute_reply.started":"2023-09-19T09:07:17.477128Z","shell.execute_reply":"2023-09-19T09:07:17.477146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\nfile_path = \"/kaggle/input/south-africa-gdelt-riots-2021/Query_roits_2021_test6_allColumns.csv\"\ndf = pd.read_csv(file_path)\n\n# Group the data by SQLDATE and count the number of data points for each date\ndate_counts = df['SQLDATE'].value_counts().sort_index()\n\n# Create a line graph\nplt.figure(figsize=(12, 6))\ndate_counts.plot(kind='line', color='skyblue')\nplt.title('Number of articles per day')\nplt.xlabel('DATE')\nplt.ylabel('Count')\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:07:17.479164Z","iopub.status.idle":"2023-09-19T09:07:17.479598Z","shell.execute_reply.started":"2023-09-19T09:07:17.479381Z","shell.execute_reply":"2023-09-19T09:07:17.479399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import DateFormatter\n\n# Assuming unrest_tweets2 is your DataFrame with the 'date' column (format from (1))\n\n# Load the dataset from (2)\nfile_path = \"/kaggle/input/south-africa-gdelt-riots-2021/Query_roits_2021_test6_allColumns.csv\"\ndf = pd.read_csv(file_path)\n\n# Group the data by SQLDATE and count the number of data points for each date\ndate_counts = df['SQLDATE'].value_counts().sort_index()\n\n# Convert SQLDATE to the same datetime format as in (1)\ndate_counts.index = pd.to_datetime(date_counts.index, format='%Y%m%d')\n\n# Create a figure and axis\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Plot the first dataset (from (1))\nax.plot(tweets_per_day.index, tweets_per_day.values, marker='o', label='Number of Tweets per Day', color='skyblue')\n\n# Plot the second dataset (converted from SQLDATE)\nax.plot(date_counts.index, date_counts.values, marker='o', label='Number of GDELT Articles per Day', color='orange')\n\n# Set the x-axis format to display only the date\ndate_formatter = DateFormatter('%Y-%m-%d')\nax.xaxis.set_major_formatter(date_formatter)\n\n# Set the dates starting from the beginning of the event:\nspecific_dates = [pd.Timestamp('2021-07-09')]\ndate_range = pd.date_range(start='2021-07-10', end='2021-07-18', freq='D')\nall_dates = specific_dates + date_range.tolist()\n\nax.set_xticks(all_dates)\n\n# Rotate the x-axis labels for better visibility\nplt.xticks(rotation=90)\n\n# Set labels and title\nax.set_xlabel('Date')\nax.set_ylabel('Count')\nax.set_title('Number of Tweets and GDELT Articles per Day')\n\n# Add a legend\nax.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:07:17.481114Z","iopub.status.idle":"2023-09-19T09:07:17.481845Z","shell.execute_reply.started":"2023-09-19T09:07:17.481635Z","shell.execute_reply":"2023-09-19T09:07:17.481656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import DateFormatter\n\n# Assuming unrest_tweets2 is your DataFrame with the 'date' column (format from (1))\n\n# Load the dataset from (2)\nfile_path = \"/kaggle/input/south-africa-gdelt-riots-2021/Query_roits_2021_test6_allColumns.csv\"\ndf = pd.read_csv(file_path)\n\n# Group the data by SQLDATE and count the number of data points for each date\ndate_counts = df['SQLDATE'].value_counts().sort_index()\n\n# Convert SQLDATE to the same datetime format as in (1)\ndate_counts.index = pd.to_datetime(date_counts.index, format='%Y%m%d')\n\n# Normalize the first dataset (from (1)) to the range [0, 1]\nmax_value_tweets = tweets_per_day.max()\nnormalized_tweets = tweets_per_day / max_value_tweets\n\n# Normalize the second dataset (converted from SQLDATE) to the range [0, 1]\nmax_value_articles = date_counts.max()\nnormalized_date_counts = date_counts / max_value_articles\n\n# Create a figure and axis\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Plot the first dataset (normalized from (1)) on the left y-axis\nax.plot(tweets_per_day.index, normalized_tweets.values, marker='o', label='Normalized Number of Tweets per Day', color='skyblue')\n\n# Plot the second dataset (normalized) on the same y-axis\nax.plot(date_counts.index, normalized_date_counts.values, marker='o', label='Normalized Number of GDELT Articles per Day', color='orange')\n\n# Ensure that all dates are displayed on the x-axis\ndate_range = pd.date_range(start='2021-07-09', end='2021-07-21', freq='D')\nax.set_xticks(date_range)\n\n# Rotate the x-axis labels for better visibility\nax.tick_params(axis='x', rotation=90)\n\n# Set labels and title\nax.set_xlabel('Date')\nax.set_ylabel('Normalized Count')\nax.set_title('Comparison of Normalized Tweets and Normalized GDELT Articles per Day')\n\n# Add a legend for both datasets\nax.legend(loc='upper right')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:07:17.483208Z","iopub.status.idle":"2023-09-19T09:07:17.483626Z","shell.execute_reply.started":"2023-09-19T09:07:17.483409Z","shell.execute_reply":"2023-09-19T09:07:17.483426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Number of Mentions of the Article per day**\n\n**Definition of NumMentions:**\nNumMentions=\t(NULLABLE;\tINTEGER)\tThis is the total number of mentions of this event across all source documents during the 15 minute update in which it was first seen. Multiple references to an event within a single document also contribute to this count. This can be used as a method of assessing the “importance” of an event: the more discussion of that event, the more likely it is to be significant. The total universe of source documents and the density of events within them vary over time, so it is recommended that this field be normalized by the average or other measure of the universe of events during the time period of interest. This field is actually a composite score of the total number of raw mentions and the number of mentions extracted from reprocessed versions of each article (see the discussion for the Mentions table). NOTE: this field refers only to the first news report to mention an event and is not updated if the event is found in a different context in other news reports. It is included for legacy purposes – for more precise information on the positioning of an event, see the Mentions table.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\nfile_path = \"/kaggle/input/south-africa-gdelt-riots-2021/Query_roits_2021_test6_allColumns.csv\"\ndf = pd.read_csv(file_path)\n\n# Group the data by SQLDATE and sum the NumMentions for each date\ndate_mentions = df.groupby('SQLDATE')['NumMentions'].sum()\n\n# Create a bar graph\nplt.figure(figsize=(12, 6))\ndate_mentions.plot(kind='bar', color='skyblue')\nplt.title('Total Number of NumMentions per Day')\nplt.xlabel('SQLDATE')\nplt.ylabel('Total NumMentions')\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:07:17.485338Z","iopub.status.idle":"2023-09-19T09:07:17.486283Z","shell.execute_reply.started":"2023-09-19T09:07:17.486023Z","shell.execute_reply":"2023-09-19T09:07:17.486049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentiment Analysis ","metadata":{}},{"cell_type":"markdown","source":"**Distribution of sentiment of articles**\nThis uses the the column title AvgTone to gauge the sentiment.\n\n**Definition of AvgTone:**\nAvgTone=\t(NULLABLE;\tFLOAT)\tThis is the average “tone” of all documents containing one or more mentions of this event during the 15 minute update in which it was first seen. The score ranges from -100 (extremely negative) to +100 (extremely positive). Common values range between -10 and +10, with 0 indicating neutral. This can be used as a method of filtering the “context” of events as a subtle measure of the importance of an event and as a proxy for the “impact” of that event. For example, a riot event with a slightly negative average tone is likely to have been a minor occurrence, whereas if it had an extremely negative average tone, it suggests a far more serious occurrence. A riot with a positive score likely suggests a very minor occurrence described in the context of a more positive narrative (such as a report of an attack occurring in a discussion of improving conditions on the ground in a country and how the number of attacks per day has been greatly reduced). NOTE: this field refers only to the first news report to mention an event and is not updated if the event is found in a different context in other news reports. It is included for legacy purposes – for more precise information on the positioning of an event, see the Mentions table. NOTE: this provides only a basic tonal assessment of an article and it is recommended that users interested in emotional measures use the Mentions and Global Knowledge Graph tables to merge the complete set of 2,300 emotions and themes from the GKG GCAM system into their analysis of event records. (Source: GDELT database)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\nfile_path = \"/kaggle/input/south-africa-gdelt-riots-2021/Query_roits_2021_test6_allColumns.csv\"\ndf = pd.read_csv(file_path)\n\n# Create a histogram of AvgTone with a bin width of 1 unit\nbin_width = 1.0\nplt.figure(figsize=(12, 6))\nplt.hist(df['AvgTone'], bins=int((df['AvgTone'].max() - df['AvgTone'].min()) / bin_width), color='skyblue')\nplt.title('Average sentiment of articles')\nplt.xlabel('Average Sentiment')\nplt.ylabel('Number of Articles')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:07:17.487675Z","iopub.status.idle":"2023-09-19T09:07:17.488064Z","shell.execute_reply.started":"2023-09-19T09:07:17.487876Z","shell.execute_reply":"2023-09-19T09:07:17.487893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\nstart_date = ''\nend_date = ''\n\n# Load the dataset\nfile_path = \"/kaggle/input/south-africa-gdelt-riots-2021/Query_roits_2021_test6_allColumns.csv\"\ndf = pd.read_csv(file_path, parse_dates=['SQLDATE'])\n\n# Group the data by date and calculate the average sentiment (AvgTone) for each day\naverage_sentiment_per_day = df.groupby('SQLDATE')['AvgTone'].mean()\n\n# Plot the average sentiment over time\nplt.figure(figsize=(12, 6))\nplt.plot(average_sentiment_per_day.index, average_sentiment_per_day.values, marker='o', linestyle='-')\nplt.title('Average Sentiment per Day Over Time')\nplt.xlabel('Date')\nplt.ylabel('Average Sentiment (AvgTone)')\nplt.grid(True)\n\n# Set the date range from '2020-10-15' to '2020-11-08'\ndate_range = pd.date_range(start='2021-07-09', end='2021-07-21', freq='D')\nall_dates = date_range.tolist()\n\n# # Set the x-axis ticks to match the SQLDATE values\n# dates_to_highlight = [\"2020-11-03\", \"2020-11-07\", \"2020-10-22\"]  # Add your desired dates here\n# title_of_dates = [\"Election Day\", \"Highest Tweets\", \"Final Presidential Debate\"]\n# colors = [\"red\", \"green\", \"yellow\"]\n\n# # Calculate the positions for evenly spaced x-axis ticks\n# x_positions = [average_sentiment_per_day.index.get_loc(date) for date in dates_to_highlight]\n\n# for i, date in enumerate(dates_to_highlight):\n#     title = title_of_dates[i]\n#     color = colors[i]\n#     date_current = dates_to_highlight[i]\n#     position = pd.Timestamp(date_current)\n#     plt.axvline(x=position, color=color, linestyle='--', label=title)\n\n# Set the x-axis ticks to be all_dates and their labels\nplt.xticks(all_dates, [date.strftime('%Y-%m-%d') for date in all_dates], rotation=45)\n\nplt.xticks(rotation=80)\n# Show the plot\nplt.tight_layout()\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:07:17.489336Z","iopub.status.idle":"2023-09-19T09:07:17.489772Z","shell.execute_reply.started":"2023-09-19T09:07:17.489540Z","shell.execute_reply":"2023-09-19T09:07:17.489573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Other Metrics Exploration**","metadata":{}},{"cell_type":"markdown","source":"**Goldstein Scale of events: Unrest event**\n\n**Definintion of Goldstein Scale:**\nGoldsteinScale=\t(NULLABLE;\tFLOAT)\tEach CAMEO event code is assigned a numeric score from -10 to +10, capturing the theoretical potential impact that type of event will have on the stability of a country. This is known as the Goldstein Scale. This field specifies the Goldstein score for each event type. NOTE: this score is based on the type of event, not the specifics of the actual event record being recorded – thus two riots, one with 10 people and one with 10,000, will both receive the same Goldstein score. This can be aggregated to various levels of time resolution to yield an approximation of the stability of a location over time. (Source GDELT Database - BigQuery)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\nfile_path = \"/kaggle/input/south-africa-gdelt-riots-2021/Query_roits_2021_test6_allColumns.csv\"\ndf = pd.read_csv(file_path)\n\n# Group the data by GoldsteinScale and count the number of articles for each scale value\ngoldstein_counts = df['GoldsteinScale'].value_counts().sort_index()\n\n# Create a bar graph\nplt.figure(figsize=(12, 6))\ngoldstein_counts.plot(kind='bar', color='skyblue')\nplt.title('Number of Articles per GoldsteinScale')\nplt.xlabel('GoldsteinScale')\nplt.ylabel('Number of Articles')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:07:17.490987Z","iopub.status.idle":"2023-09-19T09:07:17.491369Z","shell.execute_reply.started":"2023-09-19T09:07:17.491184Z","shell.execute_reply":"2023-09-19T09:07:17.491201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset\nfile_path = \"/kaggle/input/south-africa-gdelt-riots-2021/Query_roits_2021_test6_allColumns.csv\"\ndf = pd.read_csv(file_path)\n\n# Calculate the percentage of the dataset with the same GoldsteinScale\ntotal_rows = len(df)\ngoldstein_value = df['GoldsteinScale'].iloc[0]  # Assuming they all have the same value\npercentage_with_same_goldstein = (df['GoldsteinScale'].value_counts().iloc[0] / total_rows) * 100\n\n# Create a table to display the result\nresult_table = pd.DataFrame({\n    'GoldsteinScale Value': [goldstein_value],\n    'Percentage of Dataset': [percentage_with_same_goldstein]\n})\n\nprint(result_table)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:07:17.492977Z","iopub.status.idle":"2023-09-19T09:07:17.493673Z","shell.execute_reply.started":"2023-09-19T09:07:17.493430Z","shell.execute_reply":"2023-09-19T09:07:17.493450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}